---
layout: post
title:  "Support Vector Machine" 
description: 광운대학교 정한울 교수님의 머신러닝 강의를 요약한 내용입니다.
date:   2021-06-20
categories: [Deep Learning]
comments: true
author : Dongkyun kim
---

8-1
## 1. SVM이란?

머신러닝 중 가장 유명하고 널리쓰이는 알고리즘이다.

구현이 간단하고, 효율성 있다는 특징이 있다.

Classification 에 주로 쓰이고, 기존 존재한 Logistic regression, clustering과 달리 boundry를 침으로써 classify한다는 특성이 있다.

### 1-1. SVM - classification boundary

![1](/assets/img/Deep_learning/210620/1.PNG)

Boundary를 칠 때, 모든 data가 아닌 애매한, 기준이 될 data만을 추출하여 boundary를 치는것이 연산상 효율적일 것이다.

기울기와 점(위치)만 정해줄 수 있다면, boundary line을 그릴 수 있을 것이다. 

따라서 기울기, 위치(점)에 대한 기준점만 주어지면 된다.

기울기와 위치를 한 번에 정하기 힘드니, 1. 기울기 고정 후 점 구하기 2. 점 고정 후 기울기 구하기로 step을 나누어 보자.

1. 기울기가 정해졌을 때, 최적의 (line)위치를 찾는 경우 : 가장 가운데에 있는 line이 desirable 할 것이다.

![1](/assets/img/Deep_learning/210620/2.PNG)
파란점, 빨간 점 두 점과 line간의 거리가 가장 긴 지점이 line이 될 것이다.

 

2. 위치가 정해졌고 기울기를 정해야 하는 경우 : 

![1](/assets/img/Deep_learning/210620/3.PNG)

파란, 빨간 점과 선 간의 왔다갔다 하는 거리가 먼 초록색 line이 보라색 line보다 좋은 criterion일 것이다.

즉 margin을 maximize하는 line일 것이다.

 
![1](/assets/img/Deep_learning/210620/4.PNG)
![1](/assets/img/Deep_learning/210620/5.PNG)


따라서 저 초록색 line을 구하려면, 가장 가까운 거리의 파랑, 빨강 점의 중점을 지나면서 그 두 점을 이은 벡터에 수직인 직선을 구하면 optimal한 boundary가 될 것이다. 

다른 점은 다 필요 없다는 의미이다.

이때 저 두 point를 support vector라고 한다.

하지만 세상에는 linearly seperable 하지 않은 data가 너무 많고, 이를 Kernel trick, 라그랑쥬 승수법을 통해 해결한다.

8-2
## 2. Kernel Trick
---

### 2-1.  linearly inseperable한 data 를 seperable하게 분리하는 방법 

![1](/assets/img/Deep_learning/210620/7.PNG)

다음과 같은 식을 통해 boundary를 linearly seperable하도록 차원을 증가시켜 주어 classify할 수 있다.

이 고차원으로 변형시켜주는 식을 mapping function Φ 라고 한다.

하지만 이렇게 차원을 무조건적으로 증가시켜주면, computational power 역시 증가하여 단점이 있을 것이다.

차원은 높이되 계산량을 높이지 않는 방법은 없을까?에서 등장한 개념이 kernel trick이다.

### 2-2 Kernel trick

![1](/assets/img/Deep_learning/210620/8.PNG)


(Q(transformed space R) >> P(original space R))
Mapping function을 통해 high dimension으로 transform된(Q차원) Φ(x)를 Φ(x')와 내적한 값은 매우 complex한 계산을 거치지만 결국 scalar값을 가질 것이다.

이를 K(x, x')라는 특정 함수를 통해 P라는 original space에서 구할 수 있다면, 연산량은 매우 적어질 것이다.

이 K라는 function을, kernel function이라 한다.

![1](/assets/img/Deep_learning/210620/9.PNG)

다음은 kernel function의 예이다.

만약 mapping function으로 x, y를 transform하고 그 두 값을 내적한 값이, original space에서 kernel function에 x, y 값을 대입만 해준 경우와 똑같다면, 연산량이 매우 줄어들 것이다.  

Mapping function이 주어지지 않았더라도, kernel function만 주어졌다면, classify가  가능하다.

우리가 계산시에는, 고차원의 내적된 값만 필요할 뿐 저차원에서 어떻게 mapping되고 ~ history는 필요가 없다.

**오로지 kernel function을 통해 계산된 (= mapping function에서 내적한) scalar값만이 중요하다.**

![1](/assets/img/Deep_learning/210620/10.PNG)

위 그림은 어떤 mapping function을 쓰냐에 따라 달라지는 kernel function들이다.

그 중 RBF kernel을 매우 자주쓰는데, 이 function이 왜 effective한지는 다음 이유와 같다.

![1](/assets/img/Deep_learning/210620/11.PNG)

kernel function이 주어졌을 때, taylor expansion을 통해 mapping function을 예측한 슬라이드이다. 

![1](/assets/img/Deep_learning/210620/12.PNG)

Ridge regression의 경우 line이 data에 너무 fit 되다보면 wiggle거리는 경우가 많은데, 이를 input data의 차원을 kernel trick을 통해 높여주면, 고차원 상에서 ridge regression이 적용된 형태일 것이므로  wiggle거림이 해소될 것이다. 

8.3
## 3. Lagrange Multiplier
