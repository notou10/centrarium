---
layout: post
title:  "Support Vector Machine" 
description: 광운대학교 정한울 교수님의 머신러닝 강의를 요약한 내용입니다.
date:   2021-06-20
categories: [Deep Learning]
comments: true
author : Dongkyun kim
---

8-1
## 1. SVM이란?

머신러닝 중 가장 유명하고 널리쓰이는 알고리즘이다.

구현이 간단하고, 효율성 있다는 특징이 있다.

Classification 에 주로 쓰이고, 기존 존재한 Logistic regression, clustering과 달리 boundry를 침으로써 classify한다는 특성이 있다.

### 1-1. SVM - classification boundary

![1](/assets/img/Deep_learning/210620/1.PNG)

Boundary를 칠 때, 모든 data가 아닌 애매한, 기준이 될 data만을 추출하여 boundary를 치는것이 연산상 효율적일 것이다.

기울기와 점(위치)만 정해줄 수 있다면, boundary line을 그릴 수 있을 것이다. 

따라서 기울기, 위치(점)에 대한 기준점만 주어지면 된다.

기울기와 위치를 한 번에 정하기 힘드니, 1. 기울기 고정 후 점 구하기 2. 점 고정 후 기울기 구하기로 step을 나누어 보자.

1. 기울기가 정해졌을 때, 최적의 (line)위치를 찾는 경우 : 가장 가운데에 있는 line이 desirable 할 것이다.

![1](/assets/img/Deep_learning/210620/2.PNG)
파란점, 빨간 점 두 점과 line간의 거리가 가장 긴 지점이 line이 될 것이다.

 

2. 위치가 정해졌고 기울기를 정해야 하는 경우 : 

![1](/assets/img/Deep_learning/210620/3.PNG)

파란, 빨간 점과 선 간의 왔다갔다 하는 거리가 먼 초록색 line이 보라색 line보다 좋은 criterion일 것이다.

즉 margin을 maximize하는 line일 것이다.

 
![1](/assets/img/Deep_learning/210620/4.PNG)
![1](/assets/img/Deep_learning/210620/5.PNG)


따라서 저 초록색 line을 구하려면, 가장 가까운 거리의 파랑, 빨강 점의 중점을 지나면서 그 두 점을 이은 벡터에 수직인 직선을 구하면 optimal한 boundary가 될 것이다. 

다른 점은 다 필요 없다는 의미이다.

이때 저 두 point를 support vector라고 한다.

하지만 세상에는 linearly seperable 하지 않은 data가 너무 많고, 이를 Kernel trick, 라그랑쥬 승수법을 통해 해결한다.

8-2
## 2. Kernel Trick
---

### 2-1.  linearly inseperable한 data 를 seperable하게 분리하는 방법 

![1](/assets/img/Deep_learning/210620/7.PNG)

다음과 같은 식을 통해 boundary를 linearly seperable하도록 차원을 증가시켜 주어 classify할 수 있다.

이 고차원으로 변형시켜주는 식을 mapping function Φ 라고 한다.

하지만 이렇게 차원을 무조건적으로 증가시켜주면, computational power 역시 증가하여 단점이 있을 것이다.

차원은 높이되 계산량을 높이지 않는 방법은 없을까?에서 등장한 개념이 kernel trick이다.

### 2-2. Kernel trick

![1](/assets/img/Deep_learning/210620/8.PNG)


(Q(transformed space R) >> P(original space R))
Mapping function을 통해 high dimension으로 transform된(Q차원) Φ(x)를 Φ(x')와 내적한 값은 매우 complex한 계산을 거치지만 결국 scalar값을 가질 것이다.

이를 K(x, x')라는 특정 함수를 통해 P라는 original space에서 구할 수 있다면, 연산량은 매우 적어질 것이다.

이 K라는 function을, kernel function이라 한다.

![1](/assets/img/Deep_learning/210620/9.PNG)

다음은 kernel function의 예이다.

만약 mapping function으로 x, y를 transform하고 그 두 값을 내적한 값이, original space에서 kernel function에 x, y 값을 대입만 해준 경우와 똑같다면, 연산량이 매우 줄어들 것이다.  

Mapping function이 주어지지 않았더라도, kernel function만 주어졌다면, classify가  가능하다.

우리가 계산시에는, 고차원의 내적된 값만 필요할 뿐 저차원에서 어떻게 mapping되고 ~ history는 필요가 없다.

**오로지 kernel function을 통해 계산된 (= mapping function에서 내적한) scalar값만이 중요하다.**

![1](/assets/img/Deep_learning/210620/10.PNG)

위 그림은 어떤 mapping function을 쓰냐에 따라 달라지는 kernel function들이다.

그 중 RBF kernel을 매우 자주쓰는데, 이 function이 왜 effective한지는 다음 이유와 같다.

![1](/assets/img/Deep_learning/210620/11.PNG)

kernel function이 주어졌을 때, taylor expansion을 통해 mapping function을 예측한 슬라이드이다. 

![1](/assets/img/Deep_learning/210620/12.PNG)

Ridge regression의 경우 line이 data에 너무 fit 되다보면 wiggle거리는 경우가 많은데, 이를 input data의 차원을 kernel trick을 통해 높여주면, 고차원 상에서 ridge regression이 적용된 형태일 것이므로  wiggle거림이 해소될 것이다. 

8.3
## 3. Lagrange Multiplier
---
### 3.1 Lagrange multiplier이란?
Contraint가 주어졌을 때 optimize하는데 매우 powerful한 tool이다.

    Implicit function expression
    한국어로는 음함수로의 표현 이다.

    y = 3x + 5(양함수) 를,

    y - 3x - 5 = 0

    꼴과 같이 f(x, y) = 0 꼴로 쓰는 것이다.

    Gradient
    Gradient란, 변수가 여러개인 함수에 대해서, 각 변수에 대해 partial differential 해주어 벡터 꼴로 나타내준 것이다.

    위의 f(x, y)를 gradient 해주면 (-3, 1)이다.

    Gradient는, 기하학 적으로 해당 함수를 가장 크게 변화시키는 방향을 의미한다.

### 3-2. Constraint가 등식일 때
![1](/assets/img/Deep_learning/210620/13.PNG)

g(x)가 constraint이고, f(x)를 maximize해야 한다고 할때, 

g(x) 와 f(x)의 gradient가 같다(평행하다)면 그 점이 최대 혹은 최소인 점일 것이다.

즉,

1. f =  λ∇g

2. constraint : g(x) = 0

를 수학적인 편의성을 위해서 동시에 써준 것이 라그랑쥬 승수법이라 할 수 있다.

![1](/assets/img/Deep_learning/210620/14.PNG)

 constraint와 objective를 merge한 식을 auxiliart function(L)이라 하고 이는 위 그림에 있다.


 위 L을, x, y 그리고 λ에 대해서 gradient 하였을 시 0인 점을 찾으면, 그 지점은 최소 혹은 최대인 점일 것이다.

 만약 constraint가 여러개 라면, 즉  g에 더해 h라는 constraint까지 만족해야 한다라면, 

![1](/assets/img/Deep_learning/210620/17.PNG)

h를 초록색, g를 연두색 평면이라 할때 두 평면의 교선이 constraint일 것이다. 

그리고 그 교선중에 f를 최대화 하는 점이 있을 것이다 (찾아야 한다)

h의 gradient, g의 gradient가 다음과 같을 때 그 gradient간의 linear combination인 gradient가, f를 최대화 하도록 만족시킬 것이다.

즉,

g(x, y, z) = 0

h(x, y, z) = 0

그리고 저 linear conbination 을 뺀 식이 0일 것이 될 때 f가 최댓값을 갖는다.


### 3-2. Constraint가 부등식일 때

![1](/assets/img/Deep_learning/210620/18.PNG)

1. f =  λ∇g

2. constraint : g(x) <= 0

에 더하여, 

3. mg(x*) = 0

4. m >= 0 

까지 만족시켜야 한다.

#### 3-2.3. 의 의미
![1](/assets/img/Deep_learning/210620/19.PNG)

f가 최소일때의 좌표를 x*라 할 때, 

조건 1 : 위 그림은 constraint g(x) <= 0 가 f가 x*를 포함한다.

따라서 f(x)에만 집중하여 minimum point를 찾아야 하므로 constraint가 의미가 없다. 

조건 2 : 반대로 아래 그림에서는 x*가 constraint에 포함되어 있지 않으므로, 즉 차선인 g(x)의 경계선(boundary)에 x^가 있을 것이다.

즉 

조건 1 : 

L = f + mg(x) 에서 g(x)를 아예 고려하지 x
(m = 0으로 두어주면 됨)

조건 2 :

g(x*) = 0
(m > 0으로 두어주면 됨, g(x*) = 0)


이 두 조건을 한 식으로 표현하면, mg(x*) = 0이다. 

#### 3-2.4. 의 의미
![1](/assets/img/Deep_learning/210620/20.PNG)

gradient는 가장 큰 값으로 변화하는 방향을 의미한다. 

즉 3.2.3 - 조건1 은 해당 되지 않고(constraint를 고려하지 않으므로 gradient 비교할수 x) 조건 2만 해당된다.

조건 2를 살펴보면, 

boundary 조건에서 minimal 조건이 찾아지는 것이기에, 내 영역 내에는 global minimal 가 없다. 

이 때  f의 관점으로 볼 때, ∇f 가 색깔 boundary 안으로 들어가는 방향이 f가 증가하는 방향이다.(boundary 바깥에 global minimum이 있으니깐)

g의 관점에서 볼 때는, g(x) > 0은 boundary 바깥 영역이다. (그림보고 이해하기)

따라서 g(x)가 증가하는 방향(∇g)은 boundary 바깥으로 나가는 방향이다.

따라서 ∇f와 ∇g는 부호가 반대이다.

∇L = ∇f + ∇mg(x) = 0에서

f, g의 부호가 반대임을 표시하려면 

∇f = -∇mg(x)로 두어준다.

만약 constraint가 여러개라면, 

![1](/assets/img/Deep_learning/210620/21.PNG)

와 같이 앞에 sigma만 붙여주면 된다.

위 조건을 KKT condition이라 부른다.

f(x)를 minmize한다는것에 꼭 주목.. maximize이면 부호 반대. 